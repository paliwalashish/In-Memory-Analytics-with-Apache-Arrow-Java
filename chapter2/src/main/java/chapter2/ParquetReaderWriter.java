/*
 * MIT License
 *
 * Copyright (c) 2024 Packt
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */
package chapter2;

import org.apache.arrow.memory.BufferAllocator;
import org.apache.arrow.memory.RootAllocator;
import org.apache.arrow.vector.BigIntVector;
import org.apache.arrow.vector.Float8Vector;
import org.apache.arrow.vector.VectorSchemaRoot;
import org.apache.arrow.vector.ipc.ArrowReader;
import org.apache.arrow.vector.types.FloatingPointPrecision;
import org.apache.arrow.vector.types.pojo.ArrowType;
import org.apache.arrow.vector.types.pojo.Field;
import org.apache.arrow.vector.types.pojo.Schema;

import java.io.IOException;
import java.nio.file.Paths;
import java.util.Arrays;

import org.apache.arrow.dataset.file.FileFormat;
import org.apache.arrow.dataset.file.FileSystemDatasetFactory;
import org.apache.arrow.dataset.jni.NativeMemoryPool;
import org.apache.arrow.dataset.scanner.ScanOptions;
import org.apache.arrow.dataset.scanner.Scanner;
import org.apache.arrow.dataset.source.Dataset;
import org.apache.arrow.dataset.source.DatasetFactory;

/**
 * Java equivalent of parquet_reader_writer.cc - demonstrates reading and writing Parquet files
 */
public class ParquetReaderWriter {

    /**
     * Read Parquet file using Arrow Dataset API
     */
    public static void readParquet(String filepath) {
        try (BufferAllocator allocator = new RootAllocator()) {
            String uri = "file://" + Paths.get(filepath).toAbsolutePath();

            ScanOptions options = new ScanOptions(/*batchSize*/ 32768);

            try (DatasetFactory datasetFactory = new FileSystemDatasetFactory(
                    allocator, NativeMemoryPool.getDefault(),
                    FileFormat.PARQUET, uri);
                 Dataset dataset = datasetFactory.finish();
                 Scanner scanner = dataset.newScan(options);
                 ArrowReader reader = scanner.scanBatches()) {

                Schema schema = reader.getVectorSchemaRoot().getSchema();
                System.out.println("Schema: " + schema);

                int totalRows = 0;
                int batchCount = 0;
                while (reader.loadNextBatch()) {
                    VectorSchemaRoot root = reader.getVectorSchemaRoot();
                    totalRows += root.getRowCount();
                    batchCount++;

                    if (batchCount == 1) {
                        // Print first few rows of first batch
                        System.out.println("\nFirst batch preview:");
                        printBatchPreview(root, 5);
                    }
                }
                System.out.println("\nTotal batches: " + batchCount);
                System.out.println("Total rows read: " + totalRows);
            }
        } catch (Exception e) {
            System.err.println("Error reading Parquet file: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Print preview of batch data
     */
    private static void printBatchPreview(VectorSchemaRoot root, int maxRows) {
        int rowsToPrint = Math.min(root.getRowCount(), maxRows);
        System.out.println(root.contentToTSVString().split("\n", rowsToPrint + 2)[0]); // Header
        String[] lines = root.contentToTSVString().split("\n");
        for (int i = 1; i <= rowsToPrint && i < lines.length; i++) {
            System.out.println(lines[i]);
        }
        if (root.getRowCount() > maxRows) {
            System.out.println("... (" + (root.getRowCount() - maxRows) + " more rows)");
        }
    }

    /**
     * Write Parquet file
     * Note: Arrow Java Dataset API supports writing Parquet files
     */
    public static void writeParquet(String inputPath, String outputPath) {
        System.out.println("\n=== Writing Parquet ===");

        try (BufferAllocator allocator = new RootAllocator()) {
            // First, read the source data
            String uri = "file://" + Paths.get(inputPath).toAbsolutePath();

            ScanOptions options = new ScanOptions(/*batchSize*/ 32768);

            try (DatasetFactory datasetFactory = new FileSystemDatasetFactory(
                    allocator, NativeMemoryPool.getDefault(),
                    FileFormat.PARQUET, uri);
                 Dataset dataset = datasetFactory.finish();
                 Scanner scanner = dataset.newScan(options);
                 ArrowReader reader = scanner.scanBatches()) {

                System.out.println("Source schema: " + reader.getVectorSchemaRoot().getSchema());

                // In Arrow Java, writing Parquet requires setting up FileSystemDataset write
                // This is a conceptual example of the write configuration
                System.out.println("To write Parquet in Java Arrow, use:");
                System.out.println("1. org.apache.arrow.dataset.file.FileSystemDataset for Dataset writes");
                System.out.println("2. Or integrate with Apache Parquet Java library directly");

                // Count rows that would be written
                int totalRows = 0;
                while (reader.loadNextBatch()) {
                    totalRows += reader.getVectorSchemaRoot().getRowCount();
                }
                System.out.println("Would write " + totalRows + " rows to " + outputPath);
            }
        } catch (Exception e) {
            System.err.println("Error: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Create sample Arrow data and demonstrate Parquet concepts
     */
    public static void demonstrateParquetConcepts() {
        System.out.println("\n=== Parquet Concepts Demo ===");

        try (BufferAllocator allocator = new RootAllocator()) {
            // Create a sample schema
            Schema schema = new Schema(Arrays.asList(
                Field.nullable("id", new ArrowType.Int(64, true)),
                Field.nullable("value", new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)),
                Field.nullable("name", new ArrowType.Utf8())
            ));

            try (VectorSchemaRoot root = VectorSchemaRoot.create(schema, allocator)) {
                BigIntVector idVector = (BigIntVector) root.getVector("id");
                Float8Vector valueVector = (Float8Vector) root.getVector("value");
                org.apache.arrow.vector.VarCharVector nameVector =
                    (org.apache.arrow.vector.VarCharVector) root.getVector("name");

                // Populate sample data
                int numRows = 5;
                idVector.allocateNew(numRows);
                valueVector.allocateNew(numRows);
                nameVector.allocateNew(numRows);

                for (int i = 0; i < numRows; i++) {
                    idVector.setSafe(i, i + 1);
                    valueVector.setSafe(i, (i + 1) * 1.5);
                    nameVector.setSafe(i, ("item_" + (i + 1)).getBytes());
                }

                root.setRowCount(numRows);

                System.out.println("Sample data that would be written to Parquet:");
                System.out.println(root.contentToTSVString());
            }
        }
    }

    public static void main(String[] args) {
        String inputPath = "sample_data/train.parquet";
        String outputPath = "train_output.parquet";

        System.out.println("=== Parquet Reader ===");
        System.out.println("Reading Parquet file: " + inputPath);
        readParquet(inputPath);

        writeParquet(inputPath, outputPath);

        demonstrateParquetConcepts();
    }
}
